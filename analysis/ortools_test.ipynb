{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP求解をAmplifyAEではなくortoolsに変えた正しく実装できるかのでテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c0e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, zipfile, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ZIPを展開\n",
    "zip_path = \"/mnt/data/Leuven2_before_data.zip\"\n",
    "extract_dir = Path(\".\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "    z.extractall(extract_dir)\n",
    "\n",
    "# 初期状態 (centroid_init ではなく iteration_1 から)\n",
    "cluster_distance_map = {}\n",
    "records = []\n",
    "\n",
    "for fp in sorted(extract_dir.glob(\"**/iteration_*.json\")):\n",
    "    it = int(fp.stem.split(\"_\")[1])\n",
    "    with open(fp) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # そのイテレーションで更新されたクラスタだけ置き換える\n",
    "    for cluster in data:\n",
    "        cid = cluster[\"cluster_id\"]\n",
    "        cluster_distance_map[cid] = float(cluster.get(\"total_distance\", 0.0))\n",
    "\n",
    "    # 全クラスタの合計を計算\n",
    "    total = sum(cluster_distance_map.values())\n",
    "    records.append({\"iteration\": it, \"total_distance\": total})\n",
    "\n",
    "df = pd.DataFrame(records).sort_values(\"iteration\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c73c77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved: /home/toshiya1048/dev/QA_knap/out/ortools_test/iteration_total_route_summary.csv\n",
      "\n",
      "=== Final totals per instance ===\n",
      "  instance  total_route_distance_all_clusters(carry_forward)  num_clusters_known num_clusters_total(if_known)\n",
      "E-n101-k14                                             899.0                  10                         None\n",
      "  E-n51-k5                                             606.0                   6                         None\n",
      "   Leuven2                                          105444.0                  29                         None\n",
      "       out                                           98034.0                  41                         None\n",
      "\n",
      "--- Skipped records (for diagnostics) ---\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_0_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_1_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_2_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_3_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_4_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_5_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_6_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_7_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_8_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_9_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_10_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_11_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_12_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_13_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_14_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_15_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_16_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_17_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_18_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_19_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_20_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_21_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_22_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_23_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_24_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_25_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_26_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_27_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_28_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_29_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_30_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_31_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_32_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_33_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_34_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_35_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_36_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_37_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_38_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_39_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_40_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_41_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_42_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_43_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_44_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_1_swap_timings.json: item_45_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_2_swap_timings.json: item_0_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_2_swap_timings.json: item_1_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_2_swap_timings.json: item_2_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "[20251112_110829/Leuven2_before_data] iteration_2_swap_timings.json: item_3_missing_keys -> keys=['block_ms', 'from_cluster', 'iteration', 'move_ms', 'moved_indices', 'n_city', 'qa_ms', 'skip_reason', 'skipped', 'sum_dist_current_after', 'sum_dist_current_before', 'swap_index', 'to_cluster']\n",
      "... (461 more)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json, re, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# ←必要に応じて変更\n",
    "BASE = Path(\"/home/toshiya1048/dev/QA_knap/out/ortools_test\")\n",
    "def count_clusters_from_before_data_dir(dirpath: Path) -> int:\n",
    "   cand = list(dirpath.glob(\"*before_data.json\")) + list(dirpath.glob(\"before_data.json\"))\n",
    "   if not cand:\n",
    "       return 0\n",
    "   with open(cand[0], \"r\") as f:\n",
    "       j = json.load(f)\n",
    "   # \"cluster_数字\" を数える\n",
    "   return sum(1 for k in j.keys() if isinstance(k, str) and re.match(r\"cluster_\\d+$\", k))\n",
    "def load_iteration_records(path: Path):\n",
    "   \"\"\"\n",
    "   iteration_X.json を読み、{cluster_id:int, total_distance:float} の\n",
    "   正常レコードのみ返す。異常は reasons リストに積む。\n",
    "   \"\"\"\n",
    "   ok = []\n",
    "   skipped = []\n",
    "   with open(path, \"r\") as fp:\n",
    "       data = json.load(fp)\n",
    "   # 形式ゆらぎに対応：listでなければ無視\n",
    "   if not isinstance(data, list):\n",
    "       skipped.append((path.name, \"top_not_list\", str(type(data))))\n",
    "       return ok, skipped\n",
    "   for i, rec in enumerate(data):\n",
    "       if not isinstance(rec, dict):\n",
    "           skipped.append((path.name, f\"item_{i}_not_dict\", str(type(rec))))\n",
    "           continue\n",
    "       keys = set(rec.keys())\n",
    "       if \"cluster_id\" not in rec or \"total_distance\" not in rec:\n",
    "           skipped.append((path.name, f\"item_{i}_missing_keys\", f\"keys={sorted(keys)}\"))\n",
    "           continue\n",
    "       try:\n",
    "           cid = int(rec[\"cluster_id\"])\n",
    "           td  = rec[\"total_distance\"]\n",
    "           if td is None or (isinstance(td, float) and (math.isnan(td) or math.isinf(td))):\n",
    "               skipped.append((path.name, f\"item_{i}_bad_total_distance\", f\"value={td}\"))\n",
    "               continue\n",
    "           td = float(td)\n",
    "           ok.append({\"cluster_id\": cid, \"total_distance\": td})\n",
    "       except Exception as e:\n",
    "           skipped.append((path.name, f\"item_{i}_coerce_error\", repr(e)))\n",
    "   return ok, skipped\n",
    "def analyze_instance(inst_dir: Path):\n",
    "   # 例: /.../20251112_110829/Leuven2_before_data\n",
    "   instance = inst_dir.name.replace(\"_before_data\", \"\")\n",
    "   n_clusters = count_clusters_from_before_data_dir(inst_dir)\n",
    "   it_files = sorted(\n",
    "       [p for p in inst_dir.glob(\"iteration_*.json\") if p.is_file()],\n",
    "       key=lambda p: int(p.stem.split(\"_\")[1]) if \"_\" in p.stem and p.stem.split(\"_\")[1].isdigit() else 0\n",
    "   )\n",
    "   last_dist = {}         # cluster_id -> distance（持ち回り）\n",
    "   rows = []\n",
    "   all_skipped = []\n",
    "   for f in it_files:\n",
    "       it = int(f.stem.split(\"_\")[1]) if \"_\" in f.stem and f.stem.split(\"_\")[1].isdigit() else 0\n",
    "       ok, skipped = load_iteration_records(f)\n",
    "       all_skipped.extend(skipped)\n",
    "       touched = set()\n",
    "       for rec in ok:\n",
    "           cid = rec[\"cluster_id\"]\n",
    "           td  = rec[\"total_distance\"]\n",
    "           last_dist[cid] = td\n",
    "           touched.add(cid)\n",
    "       total_known = sum(last_dist.values())\n",
    "       rows.append({\n",
    "           \"instance\": instance,\n",
    "           \"iteration\": it,\n",
    "           \"total_route_distance_all_clusters(carry_forward)\": total_known,\n",
    "           \"num_touched_in_this_iter\": len(touched),\n",
    "           \"num_clusters_known\": len(last_dist),\n",
    "           \"num_clusters_total(if_known)\": n_clusters if n_clusters else None,\n",
    "       })\n",
    "   df = pd.DataFrame(rows)\n",
    "   return df, all_skipped\n",
    "def main():\n",
    "   all_rows = []\n",
    "   skipped_log = []\n",
    "   # .../<timestamp>/*_before_data を横断\n",
    "   for ts_dir in sorted([d for d in BASE.iterdir() if d.is_dir()]):\n",
    "       for inst_dir in sorted(ts_dir.glob(\"*_before_data\")):\n",
    "           df, sk = analyze_instance(inst_dir)\n",
    "           if not df.empty:\n",
    "               all_rows.append(df)\n",
    "           skipped_log.extend([(ts_dir.name, inst_dir.name, *x) for x in sk])\n",
    "   if not all_rows:\n",
    "       print(\"No iteration_X.json with usable records was found.\")\n",
    "       return\n",
    "   out = pd.concat(all_rows, ignore_index=True)\n",
    "   out = out.sort_values([\"instance\", \"iteration\"])\n",
    "   out[\"delta_from_prev\"] = out.groupby(\"instance\")[\"total_route_distance_all_clusters(carry_forward)\"].diff()\n",
    "   out[\"improvement_rate_%\"] = (\n",
    "       -100.0 * out[\"delta_from_prev\"] /\n",
    "       out.groupby(\"instance\")[\"total_route_distance_all_clusters(carry_forward)\"].shift(1)\n",
    "   )\n",
    "   csv_path = BASE / \"iteration_total_route_summary.csv\"\n",
    "   out.to_csv(csv_path, index=False)\n",
    "   print(f\"✅ saved: {csv_path}\")\n",
    "   last = out.sort_values([\"instance\",\"iteration\"]).groupby(\"instance\").tail(1)\n",
    "   print(\"\\n=== Final totals per instance ===\")\n",
    "   print(last[[\"instance\",\"total_route_distance_all_clusters(carry_forward)\",\n",
    "               \"num_clusters_known\",\"num_clusters_total(if_known)\"]].to_string(index=False))\n",
    "   if skipped_log:\n",
    "       print(\"\\n--- Skipped records (for diagnostics) ---\")\n",
    "       for ts, inst, fname, reason, detail in skipped_log[:50]:  # 多い時は先頭50件だけ表示\n",
    "           print(f\"[{ts}/{inst}] {fname}: {reason} -> {detail}\")\n",
    "       if len(skipped_log) > 50:\n",
    "           print(f\"... ({len(skipped_log)-50} more)\")\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
